{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.nn_impl import _compute_sampled_logits, _sum_rows, sigmoid_cross_entropy_with_logits\n",
    "from tensorflow.python.ops import nn_ops, embedding_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------\n",
    "# (in the future can pass these in from the command line)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Load data\n",
    "# ------------------------------------\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "train_set_size = 55000\n",
    "num_classes = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Define variables\n",
    "# ------------------------------------\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28*28=784\n",
    "y_one_hot = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.int64, [None, 1])  # 0-9 digits recognition => 10 classes\n",
    "idx = tf.placeholder(tf.int64, [None, 1])  # data point indices\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "u = tf.Variable(tf.ones([train_set_size]) * tf.log(num_classes)) # Initialize u_i = log(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Loss functions\n",
    "# ------------------------------------\n",
    "\n",
    "# Softmax without sampling\n",
    "pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                             reduction_indices=1))\n",
    "\n",
    "# Negative sampling without sampling\n",
    "pred_negative_sampling = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "cost_negative_sampling = tf.reduce_mean(-tf.reduce_sum((\n",
    "    y * tf.log(pred_negative_sampling)\n",
    "    + (1 - y) * tf.log(1 - pred_negative_sampling)\n",
    "),\n",
    "    reduction_indices=1))\n",
    "\n",
    "# Sampled softmax = Importance sampling\n",
    "cost_sampled_softmax = tf.nn.sampled_softmax_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# Noise Contrastive Estimation\n",
    "cost_nce = tf.nn.nce_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# One vs Each\n",
    "cost_ove = custom_sampled_loss(OVE)(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "def debais_cost_fn(W, b, x, y):\n",
    "    \n",
    "    pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                                 reduction_indices=1))\n",
    "    def f1(): return 0.0*cost_softmax\n",
    "    def f2(): return 10.0*cost_softmax\n",
    "    return tf.cond(tf.less(tf.random_uniform([]), tf.constant(0.999)), f1, f2)\n",
    "\n",
    "debais_cost = debais_cost_fn(W, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ld_loss(weights,\n",
    "                         biases,\n",
    "                         datapoint_weights,\n",
    "                         labels,\n",
    "                         inputs,\n",
    "                         idx,\n",
    "                         num_sampled,\n",
    "                         num_classes,\n",
    "                         num_true=1,\n",
    "                         sampled_values=None,\n",
    "                         remove_accidental_hits=True,\n",
    "                         partition_strategy=\"mod\",\n",
    "                         name=\"ld_loss\"):\n",
    "\n",
    "    logits, labels = _compute_sampled_logits(\n",
    "      weights=weights,\n",
    "      biases=biases,\n",
    "      labels=labels,\n",
    "      inputs=inputs,\n",
    "      num_sampled=num_sampled,\n",
    "      num_classes=num_classes,\n",
    "      num_true=num_true,\n",
    "      sampled_values=sampled_values,\n",
    "      subtract_log_q=False,\n",
    "      remove_accidental_hits=remove_accidental_hits,\n",
    "      partition_strategy=partition_strategy)\n",
    "    \n",
    "    \n",
    "    sampled_dp_weight = tf.transpose(embedding_ops.embedding_lookup(\n",
    "        datapoint_weights, idx, partition_strategy=partition_strategy))\n",
    "    \n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "    # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "    # \n",
    "    # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "    return sampled_dp_weight + tf.exp(-sampled_dp_weight) * _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# Learned Denominator\n",
    "cost_ld = ld_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         datapoint_weights=u,\n",
    "                         inputs=x,\n",
    "                         idx=idx,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "clip_u = u.assign(tf.maximum(0., u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Optimization started!\n",
      "Epoch: 0001 cost= 0.493090546\n",
      "Epoch: 0002 cost= 0.293873282\n",
      "Epoch: 0003 cost= 0.261314185\n",
      "Epoch: 0004 cost= 0.254083936\n",
      "Epoch: 0005 cost= 0.262350118\n",
      "Epoch: 0006 cost= 0.257801616\n",
      "Epoch: 0007 cost= 0.238646907\n",
      "Epoch: 0008 cost= 0.243439019\n",
      "Epoch: 0009 cost= 0.229165989\n",
      "Epoch: 0010 cost= 0.235063174\n",
      "Optimization Finished!\n",
      "Accuracy: 0.896\n",
      "[ 0.          2.30258512  2.30258512 ...,  2.30258512  2.30258512\n",
      "  2.30258512]\n"
     ]
    }
   ],
   "source": [
    "# Minimize error using cost\n",
    "cost = cost_ld\n",
    "sampled_loss = (cost in {cost_nce, cost_sampled_softmax, cost_ove, cost_ld})\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    print(\"Initializing\")\n",
    "    sess.run(init)\n",
    "\n",
    "    print(\"Optimization started!\")\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            if sampled_loss:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y_int: np.argmax(batch_ys, axis=1).reshape((-1,1)),\n",
    "                                                             idx: 0*np.ones(batch_xs.shape[0])[:,None]})\n",
    "                sess.run(clip_u)\n",
    "                c = np.mean(c)  # Average loss over the batch\n",
    "\n",
    "            else:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Print results\n",
    "    # ------------------------------------\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "    print(u.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stable_logistic(x):\n",
    "    \"\"\"Calculates log(1+exp(x)) in a stable way.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    return tf.maximum(x, 0.0) + tf.log(1.0   + tf.exp(-tf.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OVE(labels, logits):\n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    return _sum_rows((1 - labels) * stable_logistic(logit_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_sigmoid_cross_entropy_with_logits(labels, logits):\n",
    "    \"\"\"My implementation of nn_ops.softmax_cross_entropy_with_logits\n",
    "    Used to make sure I can do this right\"\"\"\n",
    "    return _sum_rows(tf.maximum(logits, 0.0) - logits * labels + tf.log(1.0 + tf.exp(-abs(logits))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_sampled_loss(custom_loss_function):\n",
    "    def loss(weights,\n",
    "                             biases,\n",
    "                             labels,\n",
    "                             inputs,\n",
    "                             num_sampled,\n",
    "                             num_classes,\n",
    "                             num_true=1,\n",
    "                             sampled_values=None,\n",
    "                             remove_accidental_hits=True,\n",
    "                             partition_strategy=\"mod\",\n",
    "                             name=\"ove_loss\"):\n",
    "\n",
    "        logits, labels = _compute_sampled_logits(\n",
    "          weights=weights,\n",
    "          biases=biases,\n",
    "          labels=labels,\n",
    "          inputs=inputs,\n",
    "          num_sampled=num_sampled,\n",
    "          num_classes=num_classes,\n",
    "          num_true=num_true,\n",
    "          sampled_values=sampled_values,\n",
    "          subtract_log_q=False,\n",
    "          remove_accidental_hits=remove_accidental_hits,\n",
    "          partition_strategy=partition_strategy,\n",
    "          name=name)\n",
    "        return custom_loss_function(labels=labels, logits=logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16bcbffdf091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# b = tf.zeros([10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# logits = tf.constant([[5.8, 3.0, 4.0],\n",
    "#                       [2.0, 6.0, 1.0]])\n",
    "# labels = tf.constant([[0.0, 1.0, 0.0],\n",
    "#                       [1.0, 0.0, 0.0]])\n",
    "\n",
    "# a = tf.range(30, dtype=tf.float32) + 100.0\n",
    "# W = tf.zeros([784, 10])\n",
    "# b = tf.zeros([10])\n",
    "    \n",
    "batch_xs, batch_ys = mnist.train.next_batch(1)\n",
    "print(batch_xs)\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "# print(W.eval())\n",
    "# print(b.eval())\n",
    "# print(logits.eval())\n",
    "# print(labels.eval())\n",
    "\n",
    "# logits, labels = _compute_sampled_logits(\n",
    "#   weights=tf.transpose(W),\n",
    "#   biases=b,\n",
    "#   labels=tf.convert_to_tensor(np.argmax(batch_ys, axis=1).reshape((-1,1)), dtype=tf.int64),\n",
    "#   inputs=tf.convert_to_tensor(batch_xs, dtype=tf.float32),\n",
    "#   num_sampled=5,\n",
    "#   num_classes=10,\n",
    "#  num_true=1,\n",
    "#  sampled_values=None,\n",
    "#  remove_accidental_hits=True,\n",
    "#  partition_strategy=\"mod\")\n",
    "\n",
    "#     sampled_dp_weight = embedding_ops.embedding_lookup(\n",
    "#         datapoint_weights, idx, partition_strategy=partition_strategy)\n",
    "    \n",
    "#     true_logit = _sum_rows(labels*logits)\n",
    "#     repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "#     logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "#     # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "#     # \n",
    "#     # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "#     return sampled_dp_weight + _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# # Sampled softmax\n",
    "# print(nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits).eval())\n",
    "# print(my_sigmoid_cross_entropy_with_logits(labels, logits).eval())\n",
    "# print(my_OVE(labels, logits).eval())\n",
    "\n",
    "# print(tf.random_uniform([]).shape)\n",
    "# print(tf.random_uniform([]).eval())\n",
    "# print(tf.constant(0.7).shape)\n",
    "# print(tf.random_uniform([1]).eval())\n",
    "\n",
    "# print(a.eval())\n",
    "# print(embedding_ops.embedding_lookup(a, 0).eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Defines how all data is to be loaded\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices[:, None]]\n",
    "\n",
    "\n",
    "def loadLIBSVMdata(file_path, train_test_split):\n",
    "    # Load the data\n",
    "    data = load_svmlight_file(file_path, multilabel=True)\n",
    "\n",
    "    # Separate into x and y\n",
    "    # Remove data with no y value\n",
    "    # and if multiple y values, take the first one\n",
    "    y = data[1]\n",
    "    y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "    y = np.array([y[i][0] for i in y_not_empty])\n",
    "    x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "    # Find point to split training and test sets\n",
    "    n_samples = len(y)\n",
    "    split_point = int(train_test_split * n_samples)\n",
    "\n",
    "    # Create train and test sets\n",
    "    train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "    test = MNLDataset(x[split_point:, :], y[split_point:])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_data(dataset_name, train_test_split):\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "        train = MNLDataset(mnist.train.images, mnist.train.labels) #[:,None]\n",
    "        test = MNLDataset(mnist.test.images, mnist.test.labels) #[:,None]\n",
    "    if dataset_name in {'Bibtex'}:\n",
    "        file_path = '/Users/francoisfagan/Documents/UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "        train, test = loadLIBSVMdata(file_path, train_test_split)\n",
    "\n",
    "    dim = train.x.shape[1]\n",
    "    num_classes = int(max(train.y)) + 1 #train.y.shape[1]#\n",
    "    num_train_points = train.x.shape[0]\n",
    "    return [train, test, dim, num_classes, num_train_points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "minst2 = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = MNLDataset(minst2.train.images, minst2.train.labels[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_xs, batch_ys, batch_idx = train.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(batch_ys[:,0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'Bibtex'\n",
    "file_path = '/Users/francoisfagan/Documents/UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "data = load_svmlight_file(file_path, multilabel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[1]\n",
    "y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "y = np.array([y[i][0] for i in y_not_empty])[:, None]\n",
    "x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "# Find point to split training and test sets\n",
    "n_samples = len(y)\n",
    "split_point = int(train_test_split * n_samples)\n",
    "\n",
    "# Create train and test sets\n",
    "train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "test = MNLDataset(x[split_point:, :], y[split_point:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  48.],\n",
       "       [  75.],\n",
       "       [  52.],\n",
       "       ..., \n",
       "       [ 131.],\n",
       "       [ 119.],\n",
       "       [  13.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Defines how all data is to be loaded\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices[:, None]]\n",
    "\n",
    "\n",
    "def loadLIBSVMdata(file_path, train_test_split):\n",
    "    # Load the data\n",
    "    data = load_svmlight_file(file_path, multilabel=True)\n",
    "\n",
    "    # Separate into x and y\n",
    "    # Remove data with no y value\n",
    "    # and if multiple y values, take the first one\n",
    "    y = data[1]\n",
    "    y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "    y = np.array([int(y[i][0]) for i in y_not_empty])[:, None]\n",
    "    x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "    # Find point to split training and test sets\n",
    "    n_samples = len(y)\n",
    "    split_point = int(train_test_split * n_samples)\n",
    "\n",
    "    # Create train and test sets\n",
    "    train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "    test = MNLDataset(x[split_point:, :], y[split_point:])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_data(dataset_name, train_test_split):\n",
    "    print('Loading data')\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "        train = MNLDataset(mnist.train.images, mnist.train.labels[:,None]) #\n",
    "        test = MNLDataset(mnist.test.images, mnist.test.labels[:,None]) #[:,None]\n",
    "    if dataset_name in {'Bibtex', 'Delicious', 'Eurlex'}:\n",
    "        file_path = '../UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "        train, test = loadLIBSVMdata(file_path, train_test_split)\n",
    "\n",
    "    dim = train.x.shape[1]\n",
    "    num_classes = int(max(train.y)) + 1\n",
    "    num_train_points = train.x.shape[0]\n",
    "    return [train, test, dim, num_classes, num_train_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train, test, dim, num_classes, num_train_points = load_data('mnist', 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "batch_xs, batch_ys, batch_idx = train.next_batch(1)\n",
    "xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "i=0\n",
    "y_i = train.y[i][0]\n",
    "y_i_one_hot = np.eye(int(num_classes))[y_i]\n",
    "x_i = train.x[i, :]\n",
    "denominator_i = (1 + np.exp(-(np.dot(x_i, W[:, y_i].eval()) + b.eval()[y_i]))\n",
    "                 * np.dot(1 - y_i_one_hot, np.exp(np.dot(x_i, W.eval()) + b.eval())))\n",
    "difference_i = abs(np.exp(u.eval()[i]) - denominator_i)\n",
    "print(difference_i)\n",
    "\n",
    "# WW = W[:,batch_ys].eval()\n",
    "# print(np.dot(xx,WW))\n",
    "# uu = u.eval()[batch_idx][0][0]\n",
    "# print(uu)\n",
    "# print(np.dot(xx,xx))\n",
    "# for i in range(train.x.shape[0]):\n",
    "#     label = np.eye(int(num_classes))[train.y[i][0]]\n",
    "#     print(1+np.exp(-np.dot(train.x[i,:],W[:,batch_ys].eval()))*np.dot(1-label,np.exp(np.dot(train.x[i,:],W.eval()))))\n",
    "\n",
    "\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_old = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_old = u.eval()[batch_idx][0][0]\n",
    "#                         print(u_old)\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_new = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_new = u.eval()[batch_idx][0][0]\n",
    "\n",
    "#                         print('dot difference:', dot_old - dot_new)\n",
    "#                         print('u difference:', u_old - u_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
