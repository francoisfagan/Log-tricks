{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.nn_impl import _compute_sampled_logits, _sum_rows, sigmoid_cross_entropy_with_logits\n",
    "from tensorflow.python.ops import nn_ops, embedding_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------\n",
    "# (in the future can pass these in from the command line)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Load data\n",
    "# ------------------------------------\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "train_set_size = 55000\n",
    "num_classes = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Define variables\n",
    "# ------------------------------------\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28*28=784\n",
    "y_one_hot = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.int64, [None, 1])  # 0-9 digits recognition => 10 classes\n",
    "idx = tf.placeholder(tf.int64, [None, 1])  # data point indices\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "u = tf.Variable(tf.ones([train_set_size]) * tf.log(num_classes)) # Initialize u_i = log(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Loss functions\n",
    "# ------------------------------------\n",
    "\n",
    "# Softmax without sampling\n",
    "pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                             reduction_indices=1))\n",
    "\n",
    "# Negative sampling without sampling\n",
    "pred_negative_sampling = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "cost_negative_sampling = tf.reduce_mean(-tf.reduce_sum((\n",
    "    y * tf.log(pred_negative_sampling)\n",
    "    + (1 - y) * tf.log(1 - pred_negative_sampling)\n",
    "),\n",
    "    reduction_indices=1))\n",
    "\n",
    "# Sampled softmax = Importance sampling\n",
    "cost_sampled_softmax = tf.nn.sampled_softmax_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# Noise Contrastive Estimation\n",
    "cost_nce = tf.nn.nce_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# One vs Each\n",
    "cost_ove = custom_sampled_loss(OVE)(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "def debais_cost_fn(W, b, x, y):\n",
    "    \n",
    "    pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                                 reduction_indices=1))\n",
    "    def f1(): return 0.0*cost_softmax\n",
    "    def f2(): return 10.0*cost_softmax\n",
    "    return tf.cond(tf.less(tf.random_uniform([]), tf.constant(0.999)), f1, f2)\n",
    "\n",
    "debais_cost = debais_cost_fn(W, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ld_loss(weights,\n",
    "                         biases,\n",
    "                         datapoint_weights,\n",
    "                         labels,\n",
    "                         inputs,\n",
    "                         idx,\n",
    "                         num_sampled,\n",
    "                         num_classes,\n",
    "                         num_true=1,\n",
    "                         sampled_values=None,\n",
    "                         remove_accidental_hits=True,\n",
    "                         partition_strategy=\"mod\",\n",
    "                         name=\"ld_loss\"):\n",
    "\n",
    "    logits, labels = _compute_sampled_logits(\n",
    "      weights=weights,\n",
    "      biases=biases,\n",
    "      labels=labels,\n",
    "      inputs=inputs,\n",
    "      num_sampled=num_sampled,\n",
    "      num_classes=num_classes,\n",
    "      num_true=num_true,\n",
    "      sampled_values=sampled_values,\n",
    "      subtract_log_q=False,\n",
    "      remove_accidental_hits=remove_accidental_hits,\n",
    "      partition_strategy=partition_strategy)\n",
    "    \n",
    "    \n",
    "    sampled_dp_weight = tf.transpose(embedding_ops.embedding_lookup(\n",
    "        datapoint_weights, idx, partition_strategy=partition_strategy))\n",
    "    \n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "    # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "    # \n",
    "    # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "    return sampled_dp_weight + tf.exp(-sampled_dp_weight) * _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# Learned Denominator\n",
    "cost_ld = ld_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         datapoint_weights=u,\n",
    "                         inputs=x,\n",
    "                         idx=idx,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "clip_u = u.assign(tf.maximum(0., u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Optimization started!\n",
      "Epoch: 0001 cost= 0.493090546\n",
      "Epoch: 0002 cost= 0.293873282\n",
      "Epoch: 0003 cost= 0.261314185\n",
      "Epoch: 0004 cost= 0.254083936\n",
      "Epoch: 0005 cost= 0.262350118\n",
      "Epoch: 0006 cost= 0.257801616\n",
      "Epoch: 0007 cost= 0.238646907\n",
      "Epoch: 0008 cost= 0.243439019\n",
      "Epoch: 0009 cost= 0.229165989\n",
      "Epoch: 0010 cost= 0.235063174\n",
      "Optimization Finished!\n",
      "Accuracy: 0.896\n",
      "[ 0.          2.30258512  2.30258512 ...,  2.30258512  2.30258512\n",
      "  2.30258512]\n"
     ]
    }
   ],
   "source": [
    "# Minimize error using cost\n",
    "cost = cost_ld\n",
    "sampled_loss = (cost in {cost_nce, cost_sampled_softmax, cost_ove, cost_ld})\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    print(\"Initializing\")\n",
    "    sess.run(init)\n",
    "\n",
    "    print(\"Optimization started!\")\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            if sampled_loss:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y_int: np.argmax(batch_ys, axis=1).reshape((-1,1)),\n",
    "                                                             idx: 0*np.ones(batch_xs.shape[0])[:,None]})\n",
    "                sess.run(clip_u)\n",
    "                c = np.mean(c)  # Average loss over the batch\n",
    "\n",
    "            else:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Print results\n",
    "    # ------------------------------------\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "    print(u.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stable_logistic(x):\n",
    "    \"\"\"Calculates log(1+exp(x)) in a stable way.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    return tf.maximum(x, 0.0) + tf.log(1.0   + tf.exp(-tf.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OVE(labels, logits):\n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    return _sum_rows((1 - labels) * stable_logistic(logit_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_sigmoid_cross_entropy_with_logits(labels, logits):\n",
    "    \"\"\"My implementation of nn_ops.softmax_cross_entropy_with_logits\n",
    "    Used to make sure I can do this right\"\"\"\n",
    "    return _sum_rows(tf.maximum(logits, 0.0) - logits * labels + tf.log(1.0 + tf.exp(-abs(logits))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_sampled_loss(custom_loss_function):\n",
    "    def loss(weights,\n",
    "                             biases,\n",
    "                             labels,\n",
    "                             inputs,\n",
    "                             num_sampled,\n",
    "                             num_classes,\n",
    "                             num_true=1,\n",
    "                             sampled_values=None,\n",
    "                             remove_accidental_hits=True,\n",
    "                             partition_strategy=\"mod\",\n",
    "                             name=\"ove_loss\"):\n",
    "\n",
    "        logits, labels = _compute_sampled_logits(\n",
    "          weights=weights,\n",
    "          biases=biases,\n",
    "          labels=labels,\n",
    "          inputs=inputs,\n",
    "          num_sampled=num_sampled,\n",
    "          num_classes=num_classes,\n",
    "          num_true=num_true,\n",
    "          sampled_values=sampled_values,\n",
    "          subtract_log_q=False,\n",
    "          remove_accidental_hits=remove_accidental_hits,\n",
    "          partition_strategy=partition_strategy,\n",
    "          name=name)\n",
    "        return custom_loss_function(labels=labels, logits=logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16bcbffdf091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# b = tf.zeros([10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# logits = tf.constant([[5.8, 3.0, 4.0],\n",
    "#                       [2.0, 6.0, 1.0]])\n",
    "# labels = tf.constant([[0.0, 1.0, 0.0],\n",
    "#                       [1.0, 0.0, 0.0]])\n",
    "\n",
    "# a = tf.range(30, dtype=tf.float32) + 100.0\n",
    "# W = tf.zeros([784, 10])\n",
    "# b = tf.zeros([10])\n",
    "    \n",
    "batch_xs, batch_ys = mnist.train.next_batch(1)\n",
    "print(batch_xs)\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "# print(W.eval())\n",
    "# print(b.eval())\n",
    "# print(logits.eval())\n",
    "# print(labels.eval())\n",
    "\n",
    "# logits, labels = _compute_sampled_logits(\n",
    "#   weights=tf.transpose(W),\n",
    "#   biases=b,\n",
    "#   labels=tf.convert_to_tensor(np.argmax(batch_ys, axis=1).reshape((-1,1)), dtype=tf.int64),\n",
    "#   inputs=tf.convert_to_tensor(batch_xs, dtype=tf.float32),\n",
    "#   num_sampled=5,\n",
    "#   num_classes=10,\n",
    "#  num_true=1,\n",
    "#  sampled_values=None,\n",
    "#  remove_accidental_hits=True,\n",
    "#  partition_strategy=\"mod\")\n",
    "\n",
    "#     sampled_dp_weight = embedding_ops.embedding_lookup(\n",
    "#         datapoint_weights, idx, partition_strategy=partition_strategy)\n",
    "    \n",
    "#     true_logit = _sum_rows(labels*logits)\n",
    "#     repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "#     logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "#     # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "#     # \n",
    "#     # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "#     return sampled_dp_weight + _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# # Sampled softmax\n",
    "# print(nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits).eval())\n",
    "# print(my_sigmoid_cross_entropy_with_logits(labels, logits).eval())\n",
    "# print(my_OVE(labels, logits).eval())\n",
    "\n",
    "# print(tf.random_uniform([]).shape)\n",
    "# print(tf.random_uniform([]).eval())\n",
    "# print(tf.constant(0.7).shape)\n",
    "# print(tf.random_uniform([1]).eval())\n",
    "\n",
    "# print(a.eval())\n",
    "# print(embedding_ops.embedding_lookup(a, 0).eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Defines how all data is to be loaded\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices[:, None]]\n",
    "\n",
    "\n",
    "def loadLIBSVMdata(file_path, train_test_split):\n",
    "    # Load the data\n",
    "    data = load_svmlight_file(file_path, multilabel=True)\n",
    "\n",
    "    # Separate into x and y\n",
    "    # Remove data with no y value\n",
    "    # and if multiple y values, take the first one\n",
    "    y = data[1]\n",
    "    y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "    y = np.array([y[i][0] for i in y_not_empty])\n",
    "    x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "    # Find point to split training and test sets\n",
    "    n_samples = len(y)\n",
    "    split_point = int(train_test_split * n_samples)\n",
    "\n",
    "    # Create train and test sets\n",
    "    train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "    test = MNLDataset(x[split_point:, :], y[split_point:])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_data(dataset_name, train_test_split):\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "        train = MNLDataset(mnist.train.images, mnist.train.labels) #[:,None]\n",
    "        test = MNLDataset(mnist.test.images, mnist.test.labels) #[:,None]\n",
    "    if dataset_name in {'Bibtex'}:\n",
    "        file_path = '/Users/francoisfagan/Documents/UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "        train, test = loadLIBSVMdata(file_path, train_test_split)\n",
    "\n",
    "    dim = train.x.shape[1]\n",
    "    num_classes = int(max(train.y)) + 1 #train.y.shape[1]#\n",
    "    num_train_points = train.x.shape[0]\n",
    "    return [train, test, dim, num_classes, num_train_points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "minst2 = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = MNLDataset(minst2.train.images, minst2.train.labels[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_xs, batch_ys, batch_idx = train.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(batch_ys[:,0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'Bibtex'\n",
    "file_path = '/Users/francoisfagan/Documents/UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "data = load_svmlight_file(file_path, multilabel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[1]\n",
    "y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "y = np.array([y[i][0] for i in y_not_empty])[:, None]\n",
    "x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "# Find point to split training and test sets\n",
    "n_samples = len(y)\n",
    "split_point = int(train_test_split * n_samples)\n",
    "\n",
    "# Create train and test sets\n",
    "train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "test = MNLDataset(x[split_point:, :], y[split_point:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  48.],\n",
       "       [  75.],\n",
       "       [  52.],\n",
       "       ..., \n",
       "       [ 131.],\n",
       "       [ 119.],\n",
       "       [  13.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Defines how all data is to be loaded\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices[:, None]]\n",
    "\n",
    "\n",
    "def loadLIBSVMdata(file_path, train_test_split):\n",
    "    # Load the data\n",
    "    data = load_svmlight_file(file_path, multilabel=True)\n",
    "\n",
    "    # Separate into x and y\n",
    "    # Remove data with no y value\n",
    "    # and if multiple y values, take the first one\n",
    "    y = data[1]\n",
    "    y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "    y = np.array([int(y[i][0]) for i in y_not_empty])[:, None]\n",
    "    x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "    # Find point to split training and test sets\n",
    "    n_samples = len(y)\n",
    "    split_point = int(train_test_split * n_samples)\n",
    "\n",
    "    # Create train and test sets\n",
    "    train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "    test = MNLDataset(x[split_point:, :], y[split_point:])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_data(dataset_name, train_test_split):\n",
    "    print('Loading data')\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "        train = MNLDataset(mnist.train.images, mnist.train.labels[:,None]) #\n",
    "        test = MNLDataset(mnist.test.images, mnist.test.labels[:,None]) #[:,None]\n",
    "    if dataset_name in {'Bibtex', 'Delicious', 'Eurlex'}:\n",
    "        file_path = '../UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "        train, test = loadLIBSVMdata(file_path, train_test_split)\n",
    "\n",
    "    dim = train.x.shape[1]\n",
    "    num_classes = int(max(train.y)) + 1\n",
    "    num_train_points = train.x.shape[0]\n",
    "    return [train, test, dim, num_classes, num_train_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train, test, dim, num_classes, num_train_points = load_data('mnist', 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "batch_xs, batch_ys, batch_idx = train.next_batch(1)\n",
    "xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "i=0\n",
    "y_i = train.y[i][0]\n",
    "y_i_one_hot = np.eye(int(num_classes))[y_i]\n",
    "x_i = train.x[i, :]\n",
    "denominator_i = (1 + np.exp(-(np.dot(x_i, W[:, y_i].eval()) + b.eval()[y_i]))\n",
    "                 * np.dot(1 - y_i_one_hot, np.exp(np.dot(x_i, W.eval()) + b.eval())))\n",
    "difference_i = abs(np.exp(u.eval()[i]) - denominator_i)\n",
    "print(difference_i)\n",
    "\n",
    "# WW = W[:,batch_ys].eval()\n",
    "# print(np.dot(xx,WW))\n",
    "# uu = u.eval()[batch_idx][0][0]\n",
    "# print(uu)\n",
    "# print(np.dot(xx,xx))\n",
    "# for i in range(train.x.shape[0]):\n",
    "#     label = np.eye(int(num_classes))[train.y[i][0]]\n",
    "#     print(1+np.exp(-np.dot(train.x[i,:],W[:,batch_ys].eval()))*np.dot(1-label,np.exp(np.dot(train.x[i,:],W.eval()))))\n",
    "\n",
    "\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_old = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_old = u.eval()[batch_idx][0][0]\n",
    "#                         print(u_old)\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_new = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_new = u.eval()[batch_idx][0][0]\n",
    "\n",
    "#                         print('dot difference:', dot_old - dot_new)\n",
    "#                         print('u difference:', u_old - u_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-855ab20953c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print(a_feed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# sess.run(a.assign(tf.range(30, dtype=tf.float32)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.range(30, dtype=tf.float32)+100.0, dtype=tf.float32)\n",
    "W = tf.zeros([784, 10])\n",
    "b = tf.zeros([10])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "# a_feed = sess.run([a],feed_dict={a: np.arange(30)-5})\n",
    "# print(a_feed)\n",
    "# sess.run(a.assign(tf.range(30, dtype=tf.float32)))\n",
    "sess.run(a.assign(tf.constant(np.arange(30, dtype=tf.float32))))\n",
    "print(a.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "num_classes = 10\n",
    "num_sampled = 5\n",
    "batch_size = 4\n",
    "batch_ys = np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 4 2 3]\n",
      " [2 3 0 0 2]\n",
      " [6 6 3 0 3]\n",
      " [0 2 0 5 3]]\n",
      "[[1 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]]\n",
      "[[ 6  3  6  4  5]\n",
      " [ 5  6  3  3  5]\n",
      " [10 10  7  4  7]\n",
      " [ 5  7  5 10  8]]\n",
      "[[6 3 6 4 5]\n",
      " [5 6 3 3 5]\n",
      " [0 0 7 4 7]\n",
      " [5 7 5 0 8]]\n"
     ]
    }
   ],
   "source": [
    "samples = randint(num_classes-1, size=(batch_size,num_sampled))\n",
    "print(samples)\n",
    "repeated_batch_ys = np.tile(batch_ys, (1, num_sampled))\n",
    "print(repeated_batch_ys)\n",
    "sum_samples = samples + repeated_batch_ys + 1\n",
    "print(sum_samples)\n",
    "samples_mod = np.mod(sum_samples,num_classes)\n",
    "print(samples_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "2.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'true_classes' of 'UniformCandidateSampler' Op has type float32 that does not match expected type of int64.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    491\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    550\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"Const_100:0\", shape=(2, 1), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-f1057907fdfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_candidate_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/candidate_sampling_ops.py\u001b[0m in \u001b[0;36muniform_candidate_sampler\u001b[0;34m(true_classes, num_true, num_sampled, unique, range_max, seed, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m   return gen_candidate_sampling_ops._uniform_candidate_sampler(\n\u001b[1;32m     79\u001b[0m       \u001b[0mtrue_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m       seed2=seed2, name=name)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_candidate_sampling_ops.py\u001b[0m in \u001b[0;36m_uniform_candidate_sampler\u001b[0;34m(true_classes, num_true, num_sampled, unique, range_max, seed, seed2, name)\u001b[0m\n\u001b[1;32m    484\u001b[0m                                 \u001b[0mnum_sampled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                                 \u001b[0mrange_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    487\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_UniformCandidateSamplerOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 513\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m               \u001b[0;31m# Update the maps with the default, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'true_classes' of 'UniformCandidateSampler' Op has type float32 that does not match expected type of int64."
     ]
    }
   ],
   "source": [
    "\n",
    "a = tf.Variable(tf.constant(1.),name=\"a\")\n",
    "b = tf.Variable(tf.constant(2.),name=\"b\")\n",
    "c = tf.Variable(tf.constant([2.,4.]),name=\"c\")\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "#     s.run(a.assign(5.0*b))\n",
    "#     print(a.eval())\n",
    "    result = a + b\n",
    "    stored  = tf.Variable(tf.constant(0.),name=\"stored_sum\")\n",
    "    assign_op=stored.assign(result)\n",
    "    _ = s.run(assign_op,{a:1.,b:2.})\n",
    "    print(stored.eval()) # ok, still 3 \n",
    "    _ = s.run(assign_op,{a:4.,b:5.})\n",
    "    print(stored.eval()) # ok, still 3 \n",
    "    print(val) # 3\n",
    "    val=s.run(result,{a:4.,b:5.})\n",
    "    print(val) # 9\n",
    "    print(stored.eval()) # ok, still 3 \n",
    "    print(c[0].eval())\n",
    "    \n",
    "    print(tf.nn.uniform_candidate_sampler(tf.constant([[1],[2]]), 1, 5, unique=True, range_max=10).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "tfc:\n",
      " [[ 1.  2.]\n",
      " [ 3.  4.]]\n",
      "npc:\n",
      " [[ 1.  2.]\n",
      " [ 3.  4.]]\n",
      "modified tfc:\n",
      " [[ 1.1  2.2]\n",
      " [ 3.1  4.2]]\n",
      "modified npc:\n",
      " [[ 1.1  2.2]\n",
      " [ 3.1  4.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "npc = np.array([[1.,2.],[3.,4.]])\n",
    "tfc = tf.Variable(npc) # Use variable \n",
    "\n",
    "row = np.array([[.1,.2]])\n",
    "\n",
    "with tf.Session() as sess:   \n",
    "    tf.initialize_all_variables().run() # need to initialize all variables\n",
    "\n",
    "    print('tfc:\\n', tfc.eval())\n",
    "    print('npc:\\n', npc)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            npc[i,j] += row[0,j]\n",
    "    tfc.assign(npc).eval() # assign_sub/assign_add is also available.\n",
    "    print('modified tfc:\\n', tfc.eval())\n",
    "    print('modified npc:\\n', npc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980297104634894595"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10**18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
