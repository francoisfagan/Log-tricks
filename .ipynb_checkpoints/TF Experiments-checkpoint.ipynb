{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.nn_impl import _compute_sampled_logits, _sum_rows, sigmoid_cross_entropy_with_logits\n",
    "from tensorflow.python.ops import nn_ops, embedding_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------\n",
    "# (in the future can pass these in from the command line)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Load data\n",
    "# ------------------------------------\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "train_set_size = 55000\n",
    "num_classes = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = np.array(x)  # Dimensions [num_examples] x [dim]\n",
    "        self.y = np.array(y)  # Dimensions [num_examples] x [1]\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices[:, None]]\n",
    "\n",
    "\n",
    "def loadLIBSVMdata(file_path, train_test_split):\n",
    "    # Load the data\n",
    "    data = load_svmlight_file(file_path, multilabel=True)\n",
    "\n",
    "    # Separate into x and y\n",
    "    # Remove data with no y value\n",
    "    # and if multiple y values, take the first one\n",
    "    y = data[1]\n",
    "    y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "    y = np.array([int(y[i][0]) for i in y_not_empty])[:, None]\n",
    "    x = data[0].toarray()[y_not_empty, :]\n",
    "\n",
    "    # Find point to split training and test sets\n",
    "    n_samples = len(y)\n",
    "    split_point = int(train_test_split * n_samples)\n",
    "\n",
    "    # Create train and test sets\n",
    "    train = MNLDataset(x[:split_point, :], y[:split_point])\n",
    "    test = MNLDataset(x[split_point:, :], y[split_point:])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_data(dataset_name, train_test_split):\n",
    "    print('Loading data')\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "        train = MNLDataset(mnist.train.images, mnist.train.labels[:,None]) #\n",
    "        test = MNLDataset(mnist.test.images, mnist.test.labels[:,None]) #[:,None]\n",
    "    if dataset_name in {'Bibtex', 'Delicious', 'Eurlex'}:\n",
    "        file_path = '../UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "        train, test = loadLIBSVMdata(file_path, train_test_split)\n",
    "\n",
    "    dim = train.x.shape[1]\n",
    "    num_classes = int(max(train.y)) + 1\n",
    "    num_train_points = train.x.shape[0]\n",
    "    return [train, test, dim, num_classes, num_train_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train, test, dim, num_classes, num_train_points = load_data('mnist', 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "batch_xs, batch_ys, batch_idx = train.next_batch(1)\n",
    "xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "i=0\n",
    "y_i = train.y[i][0]\n",
    "y_i_one_hot = np.eye(int(num_classes))[y_i]\n",
    "x_i = train.x[i, :]\n",
    "denominator_i = (1 + np.exp(-(np.dot(x_i, W[:, y_i].eval()) + b.eval()[y_i]))\n",
    "                 * np.dot(1 - y_i_one_hot, np.exp(np.dot(x_i, W.eval()) + b.eval())))\n",
    "difference_i = abs(np.exp(u.eval()[i]) - denominator_i)\n",
    "print(difference_i)\n",
    "\n",
    "# WW = W[:,batch_ys].eval()\n",
    "# print(np.dot(xx,WW))\n",
    "# uu = u.eval()[batch_idx][0][0]\n",
    "# print(uu)\n",
    "# print(np.dot(xx,xx))\n",
    "# for i in range(train.x.shape[0]):\n",
    "#     label = np.eye(int(num_classes))[train.y[i][0]]\n",
    "#     print(1+np.exp(-np.dot(train.x[i,:],W[:,batch_ys].eval()))*np.dot(1-label,np.exp(np.dot(train.x[i,:],W.eval()))))\n",
    "\n",
    "\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_old = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_old = u.eval()[batch_idx][0][0]\n",
    "#                         print(u_old)\n",
    "\n",
    "#                     if i_batch == 2:\n",
    "#                         xx = batch_xs.reshape((batch_xs.shape[1]))\n",
    "#                         dot_new = np.dot(xx, W[:,batch_ys].eval()) / np.dot(xx,xx)\n",
    "#                         u_new = u.eval()[batch_idx][0][0]\n",
    "\n",
    "#                         print('dot difference:', dot_old - dot_new)\n",
    "#                         print('u difference:', u_old - u_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogTricks:\n",
    "    def __init__(self, dim, num_classes, num_train_points):\n",
    "        self.W = np.zeros((dim, num_classes))  # np.array of dimensions [dim] x [num_classes]\n",
    "        self.u = np.zeros(num_train_points)  # np.array of dimensions [num_train_points]\n",
    "\n",
    "    def sgd_update(self, x, y, idx, sampled_classes, learning_rate):\n",
    "        \"\"\" Performs sgd update of variables\n",
    "\n",
    "        :param x: np.array of dimensions [batch_size] x [dim]\n",
    "        :param y: np.array of dimensions [batch_size] x [1]\n",
    "        :param idx: np.array of dimensions [batch_size] x [1]\n",
    "        :param sampled_classes: np.array of dimensions [num_sampled]\n",
    "        :param learning_rate: scalar\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform dimensions\n",
    "        idx_array = idx.reshape(-1)  # Change from dimension [batch_size] x [1] to dimension [batch_size]\n",
    "        y_array = y.reshape(-1)  # Change from dimension [batch_size] x [1] to dimension [batch_size]\n",
    "\n",
    "        # Find batch_size and num_sampled\n",
    "        batch_size = len(idx_array)\n",
    "        num_sampled = len(sampled_classes)\n",
    "\n",
    "        # Calculate logit_difference = x_i^\\top(w_k-w_{y_i}) for all i in idx and k in sampled_classes\n",
    "        logits_sampled = x.dot(self.W[:, sampled_classes])  # Dimensions [batch_size] x [num_sampled]\n",
    "        logits_true = np.array(\n",
    "            [np.dot(x[batch, :], self.W[:, y_array[batch]]) for batch in range(batch_size)])  # Dimensions [batch_size]\n",
    "        logit_true_repeat = np.tile(logits_true[:, None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "        logit_diff = logits_sampled - logit_true_repeat  # Dimensions [batch_size] x [num_sampled]\n",
    "\n",
    "        # Update u\n",
    "        logit_diff_max = np.max(logit_diff, axis=1)  # Dimensions [batch_size]\n",
    "        logit_diff_max_repeat = np.tile(logit_diff_max[:, None],\n",
    "                                        (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "        u_bound = logit_diff_max + np.log(np.exp(-logit_diff_max) +\n",
    "                                          np.sum(np.exp(logit_diff - logit_diff_max_repeat),\n",
    "                                                 axis=1))  # Dimensions [batch_size]\n",
    "        self.u[idx_array] = np.maximum(self.u[idx_array], u_bound)  # Dimensions [batch_size]\n",
    "\n",
    "        # SGD step\n",
    "        u_idx_repeat = np.tile(self.u[idx_array][:, None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "        exp_logit_diff_minus_u = np.exp(logit_diff - u_idx_repeat)\n",
    "        u_grad = 1 - np.exp(-self.u[idx_array]) - np.sum(exp_logit_diff_minus_u, axis=1)  # Dimensions [batch_size]\n",
    "        w_sample_grad = np.dot(exp_logit_diff_minus_u.T, x)  # Dimensions [num_sampled] x [dim]\n",
    "        w_true_grad = -x * np.sum(exp_logit_diff_minus_u, axis=1)[:, None]  # Dimensions [batch_size] x [dim]\n",
    "        # https://stackoverflow.com/questions/5795700/multiply-numpy-array-of-scalars-by-array-of-vectors\n",
    "\n",
    "        # Update variables\n",
    "        self.u[idx_array] = self.u[idx_array] - learning_rate * u_grad\n",
    "        self.W[:, sampled_classes] = self.W[:, sampled_classes] - learning_rate * w_sample_grad.T\n",
    "        self.W[:, y_array] = self.W[:, y_array] - learning_rate * w_true_grad.T\n",
    "\n",
    "    def lt_error(self, data):\n",
    "        pred = np.argmax(data.x.dot(self.W))\n",
    "        mean_error = np.mean(data.y.reshape(-1) != pred)\n",
    "        return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt = LogTricks(784, 10, 55000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = lt.u\n",
    "W = lt.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lt.W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, idx, s_c, learning_rate = \n",
    "x, y, idx = train.next_batch(3)\n",
    "learning_rate = 0.001\n",
    "s_c = np.random.randint(num_classes, size=num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [7]], dtype=uint8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 8])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 2, 8],\n",
       "       [6, 2, 8],\n",
       "       [6, 2, 8]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(s_c, (batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 2, 2],\n",
       "       [7, 7, 7]], dtype=uint8)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(y, (1, num_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (np.tile(s_c, (batch_size, 1)) == np.tile(y, (1, num_sampled))).astype('float')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt.sgd_update(x, y, idx, s_c, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "dim = 2\n",
    "num_sampled = 3\n",
    "num_train_points = 5\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(0,1,size=((dim, num_classes)))\n",
    "u = np.random.uniform(size=num_train_points)\n",
    "sampled_classes = np.random.randint(num_classes, size=num_sampled)\n",
    "x = np.random.normal(0,1,size=((batch_size, dim)))\n",
    "y = np.random.randint(num_classes, size=(batch_size, 1))\n",
    "idx = np.arange(batch_size)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 5.          5.          0.        ]\n",
      " [ 3.33333333  3.33333333  3.33333333]]\n",
      "[[ 0.3775236   0.47519457  0.        ]\n",
      " [ 0.42125175  1.94598071  0.54484913]]\n"
     ]
    }
   ],
   "source": [
    "# Transform dimensions\n",
    "idx_array = idx.reshape(-1)  # Change from dimension [batch_size] x [1] to dimension [batch_size]\n",
    "y_array = y.reshape(-1)  # Change from dimension [batch_size] x [1] to dimension [batch_size]\n",
    "\n",
    "# print(idx)\n",
    "# print(idx_array)\n",
    "# print(y)\n",
    "# print(y_array)\n",
    "\n",
    "# Find batch_size and num_sampled\n",
    "batch_size = len(idx_array)\n",
    "num_sampled = len(sampled_classes)\n",
    "# print(batch_size)\n",
    "# print(sampled_classes)\n",
    "# print(num_sampled)\n",
    "\n",
    "# Calculate logit_difference = x_i^\\top(w_k-w_{y_i}) for all i in idx and k in sampled_classes\n",
    "logits_sampled = x.dot(W[:, sampled_classes])  # Dimensions [batch_size] x [num_sampled]\n",
    "logits_true = np.array(\n",
    "    [np.dot(x[batch, :], W[:, y_array[batch]]) for batch in range(batch_size)])  # Dimensions [batch_size]\n",
    "logit_true_repeat = np.tile(logits_true[:, None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "logit_diff = logits_sampled - logit_true_repeat  # Dimensions [batch_size] x [num_sampled]\n",
    "\n",
    "# print('W:',W)\n",
    "# print('y_array:',y_array)\n",
    "# print('sampled_classes:',sampled_classes)\n",
    "# print('W[:, y_array]:',W[:, y_array[np.array(batch_size)]])\n",
    "# print('logits_sampled:',logits_sampled)\n",
    "# print([W[:, y_array[batch]] for batch in range(batch_size)])\n",
    "# print('x:', x)\n",
    "# print(logits_true)\n",
    "# print(logit_true_repeat)\n",
    "# print(logit_diff[3,2])\n",
    "# print(x[3,:].dot(W[:, sampled_classes[2]] - W[:,y_array[3]]))\n",
    "# print(logit_diff)\n",
    "\n",
    "# Calculate whether the sampled labels coincide with the true labels\n",
    "# labels[i,j] = I(y[i] == s_c[j])\n",
    "labels = (np.tile(s_c, (batch_size, 1)) == np.tile(y, (1, num_sampled))).astype('float')  # Dimensions [batch_size] x [num_sampled]\n",
    "print('labels:', labels)\n",
    "\n",
    "# Update u\n",
    "logit_diff_max = np.max(logit_diff,axis=1)  # Dimensions [batch_size]\n",
    "logit_diff_max_repeat = np.tile(logit_diff_max[:,None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "u_bound = logit_diff_max + np.log(np.exp(-logit_diff_max)+\n",
    "    np.sum((1-labels)*np.exp((logit_diff - logit_diff_max_repeat)), axis=1))  # Dimensions [batch_size]\n",
    "u[idx_array] = np.maximum(u[idx_array], u_bound)  # Dimensions [batch_size]\n",
    "\n",
    "# print(logit_diff_max)\n",
    "# print(logit_diff_max_repeat)\n",
    "# print(np.exp(logit_diff))\n",
    "# print('logit_diff - logit_diff_max_repeat;', logit_diff - logit_diff_max_repeat)\n",
    "# print('(1-labels)*(logit_diff - logit_diff_max_repeat):', (1-labels)*(logit_diff - logit_diff_max_repeat))\n",
    "# print(np.exp(logit_diff - logit_diff_max_repeat))\n",
    "# print((1-labels)*np.exp((logit_diff - logit_diff_max_repeat)))\n",
    "# print(np.sum(np.exp(logit_diff - logit_diff_max_repeat), axis=1))\n",
    "# print(np.sum((1-labels)*np.exp((logit_diff - logit_diff_max_repeat)), axis=1))\n",
    "# print(logit_diff_max + np.log(np.exp(-logit_diff_max)+\n",
    "#     np.sum(np.exp(logit_diff - logit_diff_max_repeat), axis=1)))\n",
    "# print(u_bound)\n",
    "# print(np.log(1 + np.sum(np.exp(logit_diff), axis=1)))\n",
    "# print(u_bound)\n",
    "# print(u[idx_array])\n",
    "# print(np.maximum(u[idx_array], u_bound))\n",
    "\n",
    "\n",
    "# SGD step\n",
    "num_non_true_sampled = np.sum(1-labels, axis = 1) # Dimensions [batch_size], remove sample from count if it equals the true label\n",
    "scaling_factor = float(num_classes) / num_sampled_not_true  # Dimensions [batch_size]\n",
    "scaling_factor_matrix = np.tile(scaling_factor[:,None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "u_idx_repeat = np.tile(u[idx_array][:, None], (1, num_sampled))  # Dimensions [batch_size] x [num_sampled]\n",
    "factor = scaling_factor_matrix * (1-labels) * np.exp(logit_diff - u_idx_repeat)  # Dimensions [batch_size] x [num_sampled]\n",
    "u_grad = 1 - np.exp(-u[idx_array]) - np.sum(factor, axis=1)* scaling_factor  # Dimensions [batch_size]\n",
    "w_sample_grad = np.dot(factor.T, x)  # Dimensions [num_sampled] x [dim]\n",
    "w_true_grad = -x*np.sum(factor, axis=1)[:,None]  # Dimensions [batch_size] x [dim]\n",
    "# https://stackoverflow.com/questions/5795700/multiply-numpy-array-of-scalars-by-array-of-vectors\n",
    "\n",
    "#print(u_grad)\n",
    "#  print(scaling_factor_matrix * (1-labels))\n",
    "# print(exp_logit_diff_minus_u)\n",
    "# print(num_sampled_not_true)\n",
    "# print(scaling_factor)\n",
    "# print(scaling_factor_matrix)\n",
    "\n",
    "# print(exp_logit_diff_minus_u)\n",
    "# print(logit_diff - u_idx_repeat)\n",
    "# print(np.exp(\n",
    "#     logit_diff - u_idx_repeat))\n",
    "# print(np.sum(np.exp(\n",
    "#     logit_diff - u_idx_repeat),axis=1))\n",
    "# print(np.exp(-u[idx_array]))\n",
    "# print(u_grad)\n",
    "# print(np.sum(np.exp(\n",
    "#     logit_diff - u_idx_repeat),axis=1))\n",
    "# print(np.sum(exp_logit_diff_minus_u, axis=1))\n",
    "# print(x)\n",
    "# print(x*np.sum(exp_logit_diff_minus_u, axis=1)[:,None])\n",
    "\n",
    "# # Update variables\n",
    "# u[idx_array] -= learning_rate * u_grad\n",
    "# W[:, s_c] -= -learning_rate * w_sample_grad.T\n",
    "# W[:, y_array] -= -learning_rate * w_true_grad.T\n",
    "\n",
    "# print(x)\n",
    "# print(W[:, y_array])\n",
    "# print(-w_true_grad)\n",
    "# print(W[:, s_c])\n",
    "# print(-w_sample_grad)\n",
    "# print(0.08630855 - 0.04608965 - 0.00349776 - 0.03862062 - 0.00397128) # Problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if u set equal to lower bound that its gradient is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(train.x,lt.W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.dot(train.x,lt.W),axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.y.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(np.dot(train.x,lt.W),axis=1)\n",
    "mean_error = np.mean(train.y.reshape(-1) != pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90101818181818183"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
