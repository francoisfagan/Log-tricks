{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.nn_impl import _compute_sampled_logits, _sum_rows, sigmoid_cross_entropy_with_logits\n",
    "from tensorflow.python.ops import nn_ops, embedding_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------\n",
    "# (in the future can pass these in from the command line)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Load data\n",
    "# ------------------------------------\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "train_set_size = 60000\n",
    "num_classes = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Define variables\n",
    "# ------------------------------------\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "y_int = tf.placeholder(tf.int64, [None, 1])  # 0-9 digits recognition => 10 classes\n",
    "idx = tf.placeholder(tf.int64, [None, 1])  # data point indices\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "u = tf.Variable(tf.ones([train_set_size]) * tf.log(num_classes)) # Initialize u_i = log(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Loss functions\n",
    "# ------------------------------------\n",
    "\n",
    "# Softmax without sampling\n",
    "pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                             reduction_indices=1))\n",
    "\n",
    "# Negative sampling without sampling\n",
    "pred_negative_sampling = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "cost_negative_sampling = tf.reduce_mean(-tf.reduce_sum((\n",
    "    y * tf.log(pred_negative_sampling)\n",
    "    + (1 - y) * tf.log(1 - pred_negative_sampling)\n",
    "),\n",
    "    reduction_indices=1))\n",
    "\n",
    "# Sampled softmax = Importance sampling\n",
    "cost_sampled_softmax = tf.nn.sampled_softmax_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# Noise Contrastive Estimation\n",
    "cost_nce = tf.nn.nce_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "# One vs Each\n",
    "cost_ove = custom_sampled_loss(OVE)(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         inputs=x,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "def debais_cost_fn(W, b, x, y):\n",
    "    \n",
    "    pred_softmax = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    cost_softmax = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred_softmax),\n",
    "                                                 reduction_indices=1))\n",
    "    def f1(): return 0.0*cost_softmax\n",
    "    def f2(): return 10.0*cost_softmax\n",
    "    return tf.cond(tf.less(tf.random_uniform([]), tf.constant(0.999)), f1, f2)\n",
    "\n",
    "debais_cost = debais_cost_fn(W, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ld_loss(weights,\n",
    "                         biases,\n",
    "                         datapoint_weights,\n",
    "                         labels,\n",
    "                         inputs,\n",
    "                         idx,\n",
    "                         num_sampled,\n",
    "                         num_classes,\n",
    "                         num_true=1,\n",
    "                         sampled_values=None,\n",
    "                         remove_accidental_hits=True,\n",
    "                         partition_strategy=\"mod\",\n",
    "                         name=\"ld_loss\"):\n",
    "\n",
    "    logits, labels = _compute_sampled_logits(\n",
    "      weights=weights,\n",
    "      biases=biases,\n",
    "      labels=labels,\n",
    "      inputs=inputs,\n",
    "      num_sampled=num_sampled,\n",
    "      num_classes=num_classes,\n",
    "      num_true=num_true,\n",
    "      sampled_values=sampled_values,\n",
    "      subtract_log_q=False,\n",
    "      remove_accidental_hits=remove_accidental_hits,\n",
    "      partition_strategy=partition_strategy)\n",
    "    \n",
    "    \n",
    "    sampled_dp_weight = tf.transpose(embedding_ops.embedding_lookup(\n",
    "        datapoint_weights, idx, partition_strategy=partition_strategy))\n",
    "    \n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "    # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "    # \n",
    "    # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "    return sampled_dp_weight + tf.exp(-sampled_dp_weight) * _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# Learned Denominator\n",
    "cost_ld = ld_loss(weights=tf.transpose(W),\n",
    "                         biases=b,\n",
    "                         datapoint_weights=u,\n",
    "                         inputs=x,\n",
    "                         idx=idx,\n",
    "                         labels=y_int,\n",
    "                         num_sampled=5,\n",
    "                         num_classes=10)\n",
    "\n",
    "clip_u = u.assign(tf.maximum(0., u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Optimization started!\n",
      "Epoch: 0001 cost= 0.493090546\n",
      "Epoch: 0002 cost= 0.293873282\n",
      "Epoch: 0003 cost= 0.261314185\n",
      "Epoch: 0004 cost= 0.254083936\n",
      "Epoch: 0005 cost= 0.262350118\n",
      "Epoch: 0006 cost= 0.257801616\n",
      "Epoch: 0007 cost= 0.238646907\n",
      "Epoch: 0008 cost= 0.243439019\n",
      "Epoch: 0009 cost= 0.229165989\n",
      "Epoch: 0010 cost= 0.235063174\n",
      "Optimization Finished!\n",
      "Accuracy: 0.896\n",
      "[ 0.          2.30258512  2.30258512 ...,  2.30258512  2.30258512\n",
      "  2.30258512]\n"
     ]
    }
   ],
   "source": [
    "# Minimize error using cost\n",
    "cost = cost_ld\n",
    "sampled_loss = (cost in {cost_nce, cost_sampled_softmax, cost_ove, cost_ld})\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    print(\"Initializing\")\n",
    "    sess.run(init)\n",
    "\n",
    "    print(\"Optimization started!\")\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            if sampled_loss:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y_int: np.argmax(batch_ys, axis=1).reshape((-1,1)),\n",
    "                                                             idx: 0*np.ones(batch_xs.shape[0])[:,None]})\n",
    "                sess.run(clip_u)\n",
    "                c = np.mean(c)  # Average loss over the batch\n",
    "\n",
    "            else:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                              y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Print results\n",
    "    # ------------------------------------\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "    print(u.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stable_logistic(x):\n",
    "    \"\"\"Calculates log(1+exp(x)) in a stable way.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    return tf.maximum(x, 0.0) + tf.log(1.0   + tf.exp(-tf.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OVE(labels, logits):\n",
    "    true_logit = _sum_rows(labels*logits)\n",
    "    repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "    logit_difference = logits - repeated_true_logit\n",
    "    return _sum_rows((1 - labels) * stable_logistic(logit_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_sigmoid_cross_entropy_with_logits(labels, logits):\n",
    "    \"\"\"My implementation of nn_ops.softmax_cross_entropy_with_logits\n",
    "    Used to make sure I can do this right\"\"\"\n",
    "    return _sum_rows(tf.maximum(logits, 0.0) - logits * labels + tf.log(1.0 + tf.exp(-abs(logits))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_sampled_loss(custom_loss_function):\n",
    "    def loss(weights,\n",
    "                             biases,\n",
    "                             labels,\n",
    "                             inputs,\n",
    "                             num_sampled,\n",
    "                             num_classes,\n",
    "                             num_true=1,\n",
    "                             sampled_values=None,\n",
    "                             remove_accidental_hits=True,\n",
    "                             partition_strategy=\"mod\",\n",
    "                             name=\"ove_loss\"):\n",
    "\n",
    "        logits, labels = _compute_sampled_logits(\n",
    "          weights=weights,\n",
    "          biases=biases,\n",
    "          labels=labels,\n",
    "          inputs=inputs,\n",
    "          num_sampled=num_sampled,\n",
    "          num_classes=num_classes,\n",
    "          num_true=num_true,\n",
    "          sampled_values=sampled_values,\n",
    "          subtract_log_q=False,\n",
    "          remove_accidental_hits=remove_accidental_hits,\n",
    "          partition_strategy=partition_strategy,\n",
    "          name=name)\n",
    "        return custom_loss_function(labels=labels, logits=logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# logits = tf.constant([[5.8, 3.0, 4.0],\n",
    "#                       [2.0, 6.0, 1.0]])\n",
    "# labels = tf.constant([[0.0, 1.0, 0.0],\n",
    "#                       [1.0, 0.0, 0.0]])\n",
    "\n",
    "a = tf.range(30, dtype=tf.float32) + 100.0\n",
    "W = tf.zeros([784, 10])\n",
    "b = tf.zeros([10])\n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "logits, labels = _compute_sampled_logits(\n",
    "  weights=tf.transpose(W),\n",
    "  biases=b,\n",
    "  labels=tf.convert_to_tensor(np.argmax(batch_ys, axis=1).reshape((-1,1)), dtype=tf.int64),\n",
    "  inputs=tf.convert_to_tensor(batch_xs, dtype=tf.float32),\n",
    "  num_sampled=5,\n",
    "  num_classes=10,\n",
    " num_true=1,\n",
    " sampled_values=None,\n",
    " remove_accidental_hits=True,\n",
    " partition_strategy=\"mod\")\n",
    "\n",
    "print(W.eval())\n",
    "print(b.eval())\n",
    "print(logits.eval())\n",
    "print(labels.eval())\n",
    "\n",
    "#     sampled_dp_weight = embedding_ops.embedding_lookup(\n",
    "#         datapoint_weights, idx, partition_strategy=partition_strategy)\n",
    "    \n",
    "#     true_logit = _sum_rows(labels*logits)\n",
    "#     repeated_true_logit = tf.tile(tf.reshape(true_logit, [-1, 1]), [1, tf.shape(logits)[1]] )\n",
    "#     logit_difference = logits - repeated_true_logit\n",
    "    \n",
    "#     # - sampled_dp_weight + tf.exp(sampled_dp_weight) * \n",
    "#     # \n",
    "#     # (1.0 + _sum_rows((1 - labels) * stable_logistic(logit_difference)))\n",
    "#     return sampled_dp_weight + _sum_rows((1 - labels) * stable_logistic(logit_difference))\n",
    "\n",
    "# # Sampled softmax\n",
    "# print(nn_ops.softmax_cross_entropy_with_logits(labels=labels, logits=logits).eval())\n",
    "# print(my_sigmoid_cross_entropy_with_logits(labels, logits).eval())\n",
    "# print(my_OVE(labels, logits).eval())\n",
    "\n",
    "# print(tf.random_uniform([]).shape)\n",
    "# print(tf.random_uniform([]).eval())\n",
    "# print(tf.constant(0.7).shape)\n",
    "# print(tf.random_uniform([1]).eval())\n",
    "\n",
    "# print(a.eval())\n",
    "# print(embedding_ops.embedding_lookup(a, 0).eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNLDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_examples = x.shape[0]\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        batch_indices = self.batch_index + np.arange(batch_size)\n",
    "        batch_indices = np.mod(batch_indices, self.num_examples)\n",
    "        self.batch_index = (self.batch_index + batch_size) % self.num_examples\n",
    "\n",
    "        return [self.x[batch_indices, :], self.y[batch_indices, :], batch_indices]\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "        return [MNLDataset(mnist.train.images, mnist.train.labels),\n",
    "                MNLDataset(mnist.test.images, mnist.test.labels),\n",
    "                784,\n",
    "                10.0,\n",
    "                mnist.train.num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = MNLDataset(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, bi = train.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_37:0'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(cost.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "dataset_name = 'Bibtex'\n",
    "file_path = '/Users/francoisfagan/Documents/UnbiasedSoftmaxData/LIBSVM/' + dataset_name + '.txt'\n",
    "data = load_svmlight_file(file_path, multilabel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[1]\n",
    "y_not_empty = [i for i, y_val in enumerate(y) if y_val != ()]\n",
    "y = np.array([y[i][0] for i in y_not_empty])\n",
    "x = data[0].toarray()[y_not_empty, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_not_hot = [int(i) for i in list(np.argmax(yb, axis=1).reshape((-1,1)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected int32 passed to parameter 'depth' of op 'OneHot', got 10.0 of type 'float' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    491\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    101\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 102\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    301\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 302\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got 10.0 of type 'float' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-dea7d6fb7bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_not_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m     return gen_array_ops._one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0;32m-> 2217\u001b[0;31m                                   name)\n\u001b[0m\u001b[1;32m   2218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_one_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   1891\u001b[0m   result = _op_def_lib.apply_op(\"OneHot\", indices=indices, depth=depth,\n\u001b[1;32m   1892\u001b[0m                                 \u001b[0mon_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moff_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m                                 axis=axis, name=name)\n\u001b[0m\u001b[1;32m   1894\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    497\u001b[0m                   \u001b[0;34m\"type '%s' instead.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                   (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n\u001b[0;32m--> 499\u001b[0;31m                    repr(values), type(values).__name__))\n\u001b[0m\u001b[1;32m    500\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32 passed to parameter 'depth' of op 'OneHot', got 10.0 of type 'float' instead."
     ]
    }
   ],
   "source": [
    "y_hot = tf.one_hot(y_not_hot, int(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "minst2 = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3],\n",
       "       [4],\n",
       "       ..., \n",
       "       [5],\n",
       "       [6],\n",
       "       [8]], dtype=uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minst2.train.labels[:,None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
